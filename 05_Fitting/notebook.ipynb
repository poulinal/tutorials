{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block loads packages\n",
    "\"\"\"\n",
    "import zfit # Fitting library\n",
    "import hist # Histogram library\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "plt.style.use(mplhep.style.LHCb2)\n",
    "\n",
    "# Numerical array calculations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(histogram, pdf, axis):\n",
    "    # Step 1: create binned pdf\n",
    "    binning = zfit.binned.RegularBinning(len(histogram.values()), 70, 110, name=\"mass\")\n",
    "    mass_binned = zfit.Space(\"mass\", binning=binning)\n",
    "    pdf_binned = pdf.to_binned(mass_binned)\n",
    "    # Step 2: calculate (histogram value - binned pdf value) / histogram uncertainty\n",
    "    x = histogram.axes[0].centers\n",
    "    Hunc = np.sqrt(np.where(histogram.variances()==0, np.ones(len(histogram.variances())), histogram.variances())) / histogram.sum()\n",
    "    residual = (histogram.density() - pdf_binned.pdf(x))/Hunc\n",
    "    # Step 3: plot residuals\n",
    "    for i in range(0, len(x)):\n",
    "        axis.fill_between([histogram.axes[0].edges[i],histogram.axes[0].edges[i+1]], [residual[i]]*2, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Naive\" estimate of Gaussian parameters\n",
    "Imagine you want to study the mass of a particle. In previous exercises, you have seen that you can calculate the mass of the $Z^0$ boson by looking the decay $Z^0\\to\\mu^+\\mu^-$ and taking the invariant mass of the two muons. Because of physical and experimental reasons, this mass is slightly different for each muon pair you look at. In order to measure the value of the $Z^0$ mass, you could look at 1000 $Z^0\\to\\mu^+\\mu^-$ decays and take the average mass you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This defines the two parameters for the Gaussian distribution and a variable where the Gauss is defined.\n",
    "\"\"\"\n",
    "true_Zmass = 91 #GeV\n",
    "true_Zwidth = 2.5 #GeV\n",
    "Zmass = zfit.Parameter('Zmass', true_Zmass, 0, 200, floating=True)\n",
    "Zwidth = zfit.Parameter('Zwidth', true_Zwidth, 0, 10, floating=True)\n",
    "mass = zfit.Space('mass', (70, 110))\n",
    "mass_distribution = zfit.pdf.Gauss(Zmass, Zwidth, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block generates 1000 data points from the distribution and plots them as a histogram.\n",
    "\"\"\"\n",
    "n_data = 1000\n",
    "data = mass_distribution.sample(n=n_data)\n",
    "\n",
    "H = hist.Hist(hist.axis.Regular(30, 70, 110))\n",
    "H.fill(data[\"mass\"])\n",
    "\n",
    "mplhep.histplot(H, histtype=\"errorbar\", xerr=True)\n",
    "plt.xlabel(r'Dimuon mass [GeV/$c^2$]')\n",
    "plt.ylabel(rf'Dimuon pairs / {(110-70)/30:.2f} GeV/$c^2$')\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data[\"mass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distribution of the mass is symmetric, the average is an **unbiased estimator** for the mass and gives a useful result. This is a very special case. If the distributions are more complicated, estimation like this are not as easy anymore or produce useless results (a definition of \"useful\" follows later). A much more general approach to measuring parameters is fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common way to estimate the best value for the parameters $\\mu$ and $\\sigma$ is to maximize their likelihood:\n",
    "$$\n",
    "\\mathcal{L}(\\mu, \\sigma) = \\prod_{\\text{data points }x_i} \\text{pdf}(x_i;\\mu, \\sigma)\n",
    "$$\n",
    "The likelihood is a function of the parameters and looks different for every data set! The best set of parameters maximises the likelihood. A way to think of this: good paramters make the pdf large where we have a lot of data points and small where we have few data points. As a result, there are many large factors and only few small factors in the product.\n",
    "\n",
    "Most algorithms minimize the negative logarithm of the likelihood (NLL). The internet gives all sorts of reasons why this is (historic, mathematically more convenient, computationally more convenient, ...?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The negative logarithm of the likelihood\n",
    "nll = zfit.loss.UnbinnedNLL(model=mass_distribution, data=data)\n",
    "\n",
    "# The algorithm to minimize the nll\n",
    "minimizer = zfit.minimize.Minuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting time!\n",
    "result = minimizer.minimize(nll)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it gives very similar values to just calculating the average (and the standard deviation)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mu={np.mean(data.numpy()):.3f}, sigma={np.std(data.numpy()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (a0, a1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1], 'hspace':0}, sharex=True)\n",
    "mplhep.histplot(H, histtype=\"errorbar\", xerr=True, density=True, label='Data', ax=a0) # Normalize histogram because this is a pdf\n",
    "x = np.linspace(70,110,1000)\n",
    "y = mass_distribution.pdf(x).numpy()\n",
    "a0.plot(x,y,label='Fitted Gaussian')\n",
    "a0.set_ylim(0,)\n",
    "a0.set_ylabel('Density')\n",
    "a0.legend()\n",
    "\n",
    "plot_residuals(H, mass_distribution, a1)\n",
    "a1.axhline(y=0, color='k')\n",
    "a1.set_xlabel(r'Dimuon mass [GeV/$c^2$]')\n",
    "a1.set_ylabel(r'Residual')\n",
    "a1.set_ylim(-2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are plotted for diagnostic purposes. The residual is the deviation of the model from the histogram in units of the histogram uncertainty. Here, the residuals are randomly positive or negative as expected for statistical fluctuations. We'll see an example of how this is useful later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncertainty on a measured value is at least as important as the value itself. We typically assume that the measured (ie. estimated!) value follows a Gaussian distribution and the value and uncertainty we give are the mean and width of that distribution. Most statistical rules you know and we use are based on this convention. For example the classical errorpropagation assumes \"Gaussian\" uncertainties.\n",
    "\n",
    "The uncertainty can be estimated using the likelihood function. The fastest and most naive way is to calculate the uncertainties from the Hesse matrix. This is a good approximation for cases that are close to ideal. some cases you will consider in your projects are far from ideal and require more advanced methods. Because this is no statitics lecture, we'll go with the basics for now though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spend most of our time making sure what we do is correct so that our results are reliable. When fitting, this means that we need to make sure that the parameter value is unbiased and the uncertainty is neither over- nor underestimated. We do this using a toy study where we generate 100 toy data sets and fit each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_toys = 100\n",
    "results = {\n",
    "    'Zmass' : np.zeros(n_toys)*np.nan,\n",
    "    'Zmass_err' : np.zeros(n_toys)*np.nan,\n",
    "    'Zwidth' : np.zeros(n_toys)*np.nan,\n",
    "    'Zwidth_err' : np.zeros(n_toys)*np.nan\n",
    "}\n",
    "Zmass.set_value(true_Zmass)\n",
    "Zwidth.set_value(true_Zwidth)\n",
    "\n",
    "toydata = mass_distribution.create_sampler(n=n_data)\n",
    "nll = zfit.loss.UnbinnedNLL(model=mass_distribution, data=toydata)\n",
    "\n",
    "for i in range(0,n_toys):\n",
    "    if i%int(n_toys/10)==0: print(f\"{i}/{n_toys}\")\n",
    "    toydata.resample() # create a new random toy data set from the true distribution\n",
    "    result = minimizer.minimize(nll) # fit the Gauss to the new toy\n",
    "    result.hesse() # calculate the errors\n",
    "\n",
    "    # save the values and errors\n",
    "    results['Zmass'][i] = result.params[Zmass]['value']\n",
    "    results['Zmass_err'][i] = result.params[Zmass]['hesse']['error']\n",
    "    results['Zwidth'][i] = result.params[Zwidth]['value']\n",
    "    results['Zwidth_err'][i] = result.params[Zwidth]['hesse']['error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pull of a parameter is the ratio of the difference between the true (ie. generated) value and the fitted (ie. estimated) value divided by its uncertainty:\n",
    "$$\n",
    "\\text{pull}_a = \\frac{a_\\text{estimate} - a_\\text{generate}}{\\sigma_a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_Zmass = (results['Zmass'] - true_Zmass) / results['Zmass_err']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we never measure the true value of a parameter. We only estimate it based on a sample. This means that each of the toy fits will get a different value, uncertainty, and pull value.\n",
    "\n",
    "Our results are only good if the pulls are distributed like a normal Gaussian (mean at zero, width of one). This means that the analysis method we use produces an unbiased estimate of the paramter and a Gaussian uncertainty. **If this is not true, we need to understand why and edit the analysis method accordingly!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histogram of the pulls for the mean\n",
    "H_pull_Zmass = hist.Hist(hist.axis.Regular(30, -5, 5))\n",
    "H_pull_Zmass.fill(pull_Zmass)\n",
    "\n",
    "mplhep.histplot(H_pull_Zmass, histtype=\"errorbar\", xerr=True, density=True)\n",
    "plt.xlabel(r'Pull for the $Z^0$ mass parameter')\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at plots is useful but let's fit a Gauss to this distribution to make sure it is normal.\n",
    "(Sorry for the inception vibes. It will get clearer with a more complex example later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pdf for the pulls\n",
    "mu_pulls = zfit.Parameter('mu_pull', 0, -5, 5)\n",
    "sig_pulls = zfit.Parameter('sig_pull', 1, 0, 10)\n",
    "pulls = zfit.Space('pulls', (-5, 5))\n",
    "gauss_pulls = zfit.pdf.Gauss(mu_pulls, sig_pulls, pulls)\n",
    "\n",
    "# Create the nll for the fit to the pulls of the Zmass parameter\n",
    "nll_pull_Zmass = zfit.loss.UnbinnedNLL(model=gauss_pulls, data=pull_Zmass)\n",
    "fit_pull_Zmass = minimizer.minimize(nll_pull_Zmass)\n",
    "fit_pull_Zmass.hesse()\n",
    "print(fit_pull_Zmass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! The mean and width are compatible with 0 and 1 within their uncertainties. Our fit produces \"useful\" results.\n",
    "\n",
    "Note that in this specific example, the average and the standard error on the mean would produce unbiased results too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathological results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider a more realistic case. We never know the true distribution. All we ever do is model what we see using theoretical or heuristic assumptions.\n",
    "\n",
    "For many measurements, we use muons because they leave a very nice and clean signature in the detector. However, we could use electrons to calculate the $Z^0$ mass instead! In the Standard Model, electrons and muons behave in exactly the same way, so the $Z^0$ decay to two electrons does not differ too much from the decay to two muons.\n",
    "\n",
    "However one major experimental difference between the two decays is that electrons loose a significant amount of energy due to Bremsstrahlung. This skews the mass distribution towards lower masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zmass.set_value(true_Zmass)\n",
    "Zwidth.set_value(true_Zwidth)\n",
    "\n",
    "alpha = zfit.Parameter('alpha', 1, floating=False)\n",
    "n = zfit.Parameter('n', 25, floating=False)\n",
    "mass_electrons = zfit.pdf.CrystalBall(Zmass, Zwidth, alpha, n, mass)\n",
    "\n",
    "x = np.linspace(70,110,1000)\n",
    "plt.plot(x, mass_electrons.pdf(x), label=r\"$Z^0\\to e^+e^-$\")\n",
    "plt.plot(x, mass_distribution.pdf(x), label=r\"$Z^0\\to \\mu^+\\mu^-$\")\n",
    "plt.xlabel(r'Dilepton invariant mass [GeV/$c^2$]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the pull study in this case where the true distribution is the skewed Gaussian and we try to estimate the mass by fitting the normal Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_toys = 100\n",
    "results = {\n",
    "    'Zmass' : np.zeros(n_toys)*np.nan,\n",
    "    'Zmass_err' : np.zeros(n_toys)*np.nan,\n",
    "    'Zwidth' : np.zeros(n_toys)*np.nan,\n",
    "    'Zwidth_err' : np.zeros(n_toys)*np.nan\n",
    "}\n",
    "Zmass.set_value(true_Zmass)\n",
    "Zwidth.set_value(true_Zwidth)\n",
    "\n",
    "toydata = mass_electrons.create_sampler(n=n_data) # This is the only line that changed wrt. to the other example!!\n",
    "nll = zfit.loss.UnbinnedNLL(model=mass_distribution, data=toydata)\n",
    "\n",
    "for i in range(0,n_toys):\n",
    "    if i%int(n_toys/10)==0: print(f\"{i}/{n_toys}\")\n",
    "    toydata.resample() # create a new random toy data set from the true distribution\n",
    "    result = minimizer.minimize(nll) # fit the Gauss to the new toy\n",
    "    result.hesse() # calculate the errors\n",
    "\n",
    "    # save the values and errors\n",
    "    results['Zmass'][i] = result.params[Zmass]['value']\n",
    "    results['Zmass_err'][i] = result.params[Zmass]['hesse']['error']\n",
    "    results['Zwidth'][i] = result.params[Zwidth]['value']\n",
    "    results['Zwidth_err'][i] = result.params[Zwidth]['hesse']['error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_Zmass = (results['Zmass'] - true_Zmass) / results['Zmass_err']\n",
    "# Create a pdf for the pulls\n",
    "mu_pulls = zfit.Parameter('mu_pull', 0, -10, 10)\n",
    "sig_pulls = zfit.Parameter('sig_pull', 1, 0, 20)\n",
    "pulls = zfit.Space('pulls', (-20, 20))\n",
    "gauss_pulls = zfit.pdf.Gauss(mu_pulls, sig_pulls, pulls)\n",
    "\n",
    "# Create the nll for the fit to the pulls of the Zmass parameter\n",
    "nll_pull_Zmass = zfit.loss.UnbinnedNLL(model=gauss_pulls, data=pull_Zmass)\n",
    "fit_pull_Zmass = minimizer.minimize(nll_pull_Zmass)\n",
    "fit_pull_Zmass.hesse()\n",
    "print(fit_pull_Zmass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe as you expected, this is not a normal distribution.\n",
    "\n",
    "The mean is at -6. This means that the estimate for the $Z^0$ mass is biased and on average, we underestimate it by 6 times the uncertainty.\n",
    "\n",
    "The width is coincidentally compatible with one.\n",
    "\n",
    "The residuals can help us here to find out where the model is wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (a0, a1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1], 'hspace':0}, sharex=True)\n",
    "# Histogram with the data\n",
    "H = hist.Hist(hist.axis.Regular(30, 70, 110))\n",
    "H.fill(toydata[\"mass\"])\n",
    "mplhep.histplot(H, histtype=\"errorbar\", xerr=True, density=True, label='Data', ax=a0) # Normalize histogram because this is a pdf\n",
    "x = np.linspace(70,110,1000)\n",
    "y = mass_distribution.pdf(x).numpy()\n",
    "a0.plot(x,y,label='Fitted Gaussian')\n",
    "a0.set_ylim(0,)\n",
    "a0.set_ylabel('Density')\n",
    "a0.legend()\n",
    "\n",
    "\"\"\"Calculate residuals for diagnostics\"\"\"\n",
    "plot_residuals(H, mass_distribution, a1)\n",
    "a1.axhline(y=0, color='k')\n",
    "a1.set_xlabel(r'Dimuon mass [GeV/$c^2$]')\n",
    "a1.set_ylabel(r'Residual')\n",
    "a1.set_ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifications for very large data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of time (and computation ressources) a fit needs scales with the amount of data in the sample. In order to speed up the calculations, it can be useful to perform a \"binned\" fit. Previously, we fitted the distribution to the full data set. In a binned fit, we fit the distribution to a histogram. This means that each histogram bin with its uncertainty is now considered a data point and instead of using millions of data points, the binned fit only ever sees as many data points as the number of bins in the histogram.\n",
    "\n",
    "**Mind:** when performing a binned fit, there is always a loss of information. There needs to be enough data and you need to ensure that you are choosing a suitable binning.\n",
    "\n",
    "Let's consider the $Z^0$ mass example with 200M instead of 200 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 200e6\n",
    "n_bins = 100\n",
    "\n",
    "Zmass.set_value(true_Zmass)\n",
    "Zwidth.set_value(true_Zwidth)\n",
    "data_unbinned = mass_distribution.sample(n=n_data)\n",
    "\n",
    "# Binned data and model\n",
    "binning = zfit.binned.RegularBinning(n_bins, 70, 110, name=\"mass\")\n",
    "mass_binned = zfit.Space(\"mass\", binning=binning)\n",
    "data_binned = data_unbinned.to_binned(mass_binned)\n",
    "model_binned = mass_distribution.to_binned(mass_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplhep.histplot(data_binned, histtype=\"errorbar\", xerr=True, density=True)\n",
    "plt.ylim(0,)\n",
    "plt.xlabel(r'Dimuon mass [GeV/$c^2$]')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_unbinned = zfit.loss.UnbinnedNLL(model=mass_distribution, data=data)\n",
    "nll_binned = zfit.loss.BinnedNLL(model=model_binned, data=data_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_unbinned = minimizer.minimize(nll_unbinned)\n",
    "result_unbinned.hesse()\n",
    "print(result_unbinned)\n",
    "result_binned = minimizer.minimize(nll_binned)\n",
    "result_binned.hesse()\n",
    "print(result_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are compatible within their uncertainties! That's because there is a lot of data and we chose a sensible binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended pdfs\n",
    "So far, we were only interested in the shape of the distribution. All functions were normalized to an integral of one (that's why we used \"density=True\" in the histograms). Very often however, we are interested in the amount of data. In particular when the data has more than one species, for example signal and background. In this case, we work with extended pdfs. This means pdfs that are scaled with the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zmass.set_value(true_Zmass)\n",
    "Zwidth.set_value(true_Zwidth)\n",
    "\n",
    "n_signal = 100\n",
    "N_Z = zfit.Parameter('N_Z', n_signal, 0, 5*n_signal)\n",
    "mass_distribution = mass_distribution.create_extended(N_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bkg = 100\n",
    "c = zfit.Parameter('c', -0.001, -0.01, 1)\n",
    "bkg_distribution = zfit.pdf.Exponential(obs=mass, lam=c)\n",
    "N_bkg = zfit.Parameter('N_bkg', n_bkg, 0, 5*n_bkg)\n",
    "bkg_distribution = bkg_distribution.create_extended(N_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pdf = zfit.pdf.SumPDF([mass_distribution, bkg_distribution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_pdf.sample(n=n_bkg+n_signal)\n",
    "\n",
    "# A histogram of the pulls for the mean\n",
    "H = hist.Hist(hist.axis.Regular(20, 70, 110))\n",
    "H.fill(data['mass'])\n",
    "mplhep.histplot(H, histtype=\"errorbar\", xerr=True)\n",
    "plt.ylim(0,)\n",
    "plt.xlabel(r'Dimuon mass [GeV/$c^2$]')\n",
    "plt.ylabel(rf'Dimuon pairs / {(110-70)/20:.2f} GeV/$c^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NLL needs to be extended, this adds an additional term because the normalisation is not a normal parameter\n",
    "nll_unbinned = zfit.loss.ExtendedUnbinnedNLL(model=full_pdf, data=data)\n",
    "result = minimizer.minimize(nll_unbinned)\n",
    "result.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Take the model with signal and background.\n",
    "1. Run the fit with more or less data.\n",
    "2. Set up a binned fit for this scenario. What's the minimum amount of bins you need and the minimum amount of data per bin so that you get the same result as for the unbinned fit?\n",
    "3. Run a toy study for this model (you choose the number of data points and bins and whether you want to do an unbinned or binned fit). Are the parameters unbiased and the uncertainty correct?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
